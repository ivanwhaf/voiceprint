# 声纹识别-代码说明文档

## 数据集简介
AISHELL-1中文普通话语音数据来自中国不同口音区域发音人参与录制。录制过程在安静室内环境中，
同时使用 3 种不同设备：高保真麦克风（44.1kHz，16bit）；Android 系统手机（16kHz，
16bit）；iOS 系统手机（16kHz，16bit）。共 178 小时，抽取 400 名男女平衡高保真麦克风数据，
数据格式降采样为 16kHz。数据分为训练集、开发集、测试集。训练集+开发集共380类，测试集20类。

## 项目设定
在本项目中，为了加快训练速度，我们不使用数据集的原始wav文件，而是先读入wav文件提取mel频谱特征（npy数组），再进行specaugment增强，最后保存为npy文件
这样做训练速度可以加快一倍左右。训练时，训练集中每一类选取100~300条语音共380*100约38000条语音。
模型采用ECAPA-TDNN，损失函数采用aam-softmax

## 项目各文件说明
dateset.py: AISHELL-1数据集，定义的dataset类和dataloader类
inference.py: 推理函数，用于推理数据集的分数
loss.py: 损失函数aam-softmax
model.py: ECAPA-TDNN模型
train.py: 传统的训练模式训练完整的数据集
train_online.py: 在线学习模式
train_sort.py: 按照分数排序好使用我们提出方法的训练
wav2npy.py: 将wav原始数据集转成提取完特征并增强的npy
test.py: 一些测试相关的工具函数

## 使用方式
1. 使用38000 .npy数据训练baseline，运行train.py
2. 修改数据集的数量（0~300 npy/person）训练N个性能不同的基模型
3. 运行inference.py使用训练好的基模型对数据集推理并记录下每个样本的分数，并保存为sample_loss.npy文件
4. 运行test.py中的排序代码对样本按分数大小排序并保存为sample_loss_sort.npy文件
5. 重采样并重新训练，运行train_sort.py,修改其中cut epoch 和 cut ratio

## 核心代码块
### 推理部分
``` python
            for batch_idx, (inputs, labels, paths) in enumerate(train_loader):
                inputs, labels = inputs.to(device), labels.to(device)

                outputs = model(inputs)

                criterion = AAMsoftmax(args.num_classes).to(device)
                losses, _ = criterion(outputs, labels)

                losses = losses.detach().cpu().numpy().tolist()
                labels = labels.detach().cpu().numpy().tolist()

                for j in range(len(paths)):
                    path = paths[j]
                    loss = losses[j]
                    label = labels[j]
                    if path not in sample_loss:
                        sample_loss[path] = [loss, label]
                    else:
                        x = sample_loss[path]
                        x[0] = x[0] + loss
                        sample_loss[path] = x
```

### 训练数据重采样
``` python
    for epoch in range(args.epochs):
        if epoch == 50:
            indices = np.arange(int(len(train_set) * 0.1), int(len(train_set) * 0.9))
            train_set = Subset(train_set, indices)
            train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=2)

        train_loss_lst, train_acc_lst = train(model, train_loader, optimizer, criterion, epoch, device, train_loss_lst,
                                              train_acc_lst)
```
